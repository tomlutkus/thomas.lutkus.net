[{"content":"Virtual Proxmox Lab (Part 4) - Clustering: Corosync \u0026amp; Ceph 1. Introduction Last week I did some work on the blog\u0026rsquo;s design, and I was a bit under the water, so I ended up forfeiting working on this blog post. Today I will start by doing a quick recap on how to get the lab deployed by using my GitHub files. It assumes that you have the Proxmox automated install .iso per Part 1. Then I will proceed to explain the CLI procedures for Corosync and Ceph. This part is way shorter and much more straightforward than what we\u0026rsquo;ve been doing before. I was aiming to get you to experience the process of scripting and figuring out what to do. Now we\u0026rsquo;re more practical. I don\u0026rsquo;t know if at any point I will script this. It\u0026rsquo;s possible. Maybe partially. You have a freedom to make mistakes and redeploy the whole lab in 3 minutes, use it.\n2. Recap: Setting up the Lab First, create the directory to receive the project:\nmkdir -p ~/Projects Then pull my GitHub repo:\ncd ~/Projects git clone https://github.com/tomlutkus/virtual-proxmox-lab Next, make sure that you have the proper ACLs in your /var/lib/libvirt directory (Arch in my case, adjust to the package manager of your distro), so that we don\u0026rsquo;t need to be operating as root:\n# Install the ACL package sudo pacman -S acl # Apply rwx permissions for your user to all existing files/dirs recursively sudo setfacl -R -m u:$(whoami):rwx,m::rwx /var/lib/libvirt/ # Set default ACL - new files/dirs created inside inherit these permission sudo setfacl -R -d -m u:$(whoami):rwx,m::rwx /var/lib/libvirt/ Make sure that your user is configured to connect to qemu:///system:\necho \u0026#39;export LIBVIRT_DEFAULT_URI=\u0026#34;qemu:///system\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc You must have the .iso files already. the full steps to create them are in part 1, this is just a short recap of what you need to get going with the scripts.\nNow, deploy the lab:\ncd ~/Projects/virtual-proxmox-lab/host/scripts chmod +x deploy_lab.sh ./deploy_lab.sh Watch the install run:\nfor N in pve-0{1..3}; do virt-viewer -w -r \u0026#34;${N}\u0026#34; \u0026amp; done When the VMs power off, start them again and wait for them to give you the login prompt:\nfor N in pve-0{1..3}; do virsh start \u0026#34;${N}\u0026#34;; done 3. Corosync HA This is where we wanted to get with today\u0026rsquo;s post. You now understand how to do an automated deployment of a Proxmox Virtual Cluster, and you have the ability to deploy and dismantle it at will, precisely in the same way. There are a lot of tutorials covering the next steps through the GUI, and I find that boring at this point. We should want to live in the CLI: everything is slow and so much clicking around the GUI. It\u0026rsquo;s more prone to mistakes. I will grant it: it\u0026rsquo;s great for getting started and understanding how it works.\n3.1 Checking that the Nodes Talk The first thing is to test that the nodes talk to each other. I will go the lazy path and test from just pve-01 and assume that everything is working, because it probably will, at this point:\nping -c 2 10.0.253.1 ping -c 2 10.0.253.2 ping -c 2 10.0.253.3 3.2 Creating the Corosync Cluster We will be calling it lab-cluster, but name it however you like. We will let the nodes reference each other via the wan interface on the subnet 192.168.122.0/24, but in production you\u0026rsquo;d switch that to a management internal interface.\nRun the following command on pve-01 to initialize the cluster: pvecm create lab-cluster --link0 10.0.253.1 Then check if it\u0026rsquo;s up with pvecm status for a similar output as bellow: root@pve-01:~# pvecm status Cluster information ------------------- Name: lab-cluster Config Version: 1 Transport: knet Secure auth: on Quorum information ------------------ Date: Sun Feb 15 17:55:29 2026 Quorum provider: corosync_votequorum Nodes: 1 Node ID: 0x00000001 Ring ID: 1.5 Quorate: Yes Votequorum information ---------------------- Expected votes: 1 Highest expected: 1 Total votes: 1 Quorum: 1 Flags: Quorate Membership information ---------------------- Nodeid Votes Name 0x00000001 1 10.0.253.1 (local) SSH into pve-02 run the following, replying yes to the prompt pvecm add 192.168.122.1 --link0 10.0.253.2 SSH into pve-03 run the following, replying yes to the prompt pvecm add 192.168.122.1 --link0 10.0.253.3 Back into pve-01 run check the cluster with pvecm status and corosync-cfgtool -s: Cluster information ------------------- Name: lab-cluster Config Version: 3 Transport: knet Secure auth: on Quorum information ------------------ Date: Sun Feb 15 17:57:42 2026 Quorum provider: corosync_votequorum Nodes: 3 Node ID: 0x00000001 Ring ID: 1.d Quorate: Yes Votequorum information ---------------------- Expected votes: 3 Highest expected: 3 Total votes: 3 Quorum: 2 Flags: Quorate Membership information ---------------------- Nodeid Votes Name 0x00000001 1 10.0.253.1 (local) 0x00000002 1 10.0.253.2 0x00000003 1 10.0.253.3 root@pve-01:~# corosync-cfgtool -s Local node ID 1, transport knet LINK ID 0 udp addr\t= 10.0.253.1 status: nodeid: 1:\tlocalhost nodeid: 2:\tconnected nodeid: 3:\tconnected If your output is similar to mine, your Corosync is now fully configured.\n4. Ceph I mentioned before \u0026ldquo;fully redundant\u0026rdquo;. What I mean by that is not that each node is capable of running entirely alone, but that the cluster will handle one node loss. If 2 nodes are lost, the data protection is fully redundant, but it will block all I/O to protect data integrity, until the lost node(s) are back, then Ceph resumes and re-syncs automatically. This is the default size 3, min_size 2 configuration.\n4.1 Install Ceph In each of the nodes pve-01, pve-02 and pve-03, run the following command to install Ceph:\npveceph install --repository no-subscription Answer yes to any prompt asking to about continuing.\nThen proceed on node pve-01 run this to initialize Ceph:\npveceph init --network 10.0.254.0/24 4.2 Add MONs and MGRs In each of the nodes pve-01, pve-02 and pve-03 proceed to add a MON and MGR on each:\npveceph mon create pveceph mgr create 4.3 Create OSDs and Pool Now let\u0026rsquo;s give shape to the whole thing. Create the OSDs out of the physical disks. First, check with lsblk:\nroot@pve-01:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 32G 0 disk ├─sda1 8:1 0 1007K 0 part ├─sda2 8:2 0 512M 0 part └─sda3 8:3 0 31.5G 0 part ├─pve-swap 252:2 0 3.9G 0 lvm [SWAP] ├─pve-root 252:3 0 13.8G 0 lvm / ├─pve-data_tmeta 252:4 0 1G 0 lvm │ └─pve-data 252:6 0 11.8G 0 lvm └─pve-data_tdata 252:5 0 11.8G 0 lvm └─pve-data 252:6 0 11.8G 0 lvm sdb 8:16 0 150G 0 disk sdc 8:32 0 150G 0 disk sr0 11:0 1 1024M 0 rom In my case, the drives /dev/sdb and /dev/sdc are the ones with 150GB that we set aside for Ceph. Make sure that yours are too in each node, otherwise adapt. Run on each node:\npveceph osd create /dev/sdb pveceph osd create /dev/sdc Now create the pool:\npveceph pool create ceph-pool --pg_autoscale_mode on Then add it as a Proxmox storage backend:\npvesm add rbd ceph-pool --pool ceph-pool --content images,rootdir 4.4 Validate Let\u0026rsquo;s make sure everything is healthy:\nroot@pve-01:~# ceph -s cluster: id: 193abaf8-c5a6-432b-b4ea-d86fb2587672 health: HEALTH_OK services: mon: 3 daemons, quorum pve-01,pve-02,pve-03 (age 4m) mgr: pve-01(active, since 4m), standbys: pve-02, pve-03 osd: 6 osds: 6 up (since 2m), 6 in (since 2m) data: pools: 2 pools, 129 pgs objects: 2 objects, 577 KiB usage: 161 MiB used, 900 GiB / 900 GiB avail pgs: 129 active+clean All 3 MONs in quorum, 1 active MGR with 2 standbys, 6 OSDs up and in (2 per node), all PGs active+clean. You can also verify the OSD distribution:\nroot@pve-01:~# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.87900 root default -3 0.29300 host pve-01 0 hdd 0.14650 osd.0 up 1.00000 1.00000 1 hdd 0.14650 osd.1 up 1.00000 1.00000 -5 0.29300 host pve-02 2 hdd 0.14650 osd.2 up 1.00000 1.00000 3 hdd 0.14650 osd.3 up 1.00000 1.00000 -7 0.29300 host pve-03 4 hdd 0.14650 osd.4 up 1.00000 1.00000 5 hdd 0.14650 osd.5 up 1.00000 1.00000 Finally, confirm the replication settings:\nceph osd pool get ceph-pool size ceph osd pool get ceph-pool min_size You should see size: 3 and min_size: 2. Every object is replicated across all 3 nodes. Lose 1 node and the cluster keeps running. Lose 2 and I/O blocks to protect the data, but nothing is lost: bring the node(s) back and Ceph re-syncs automatically.\n5. Conclusion That\u0026rsquo;s it. Corosync was 3 commands. Ceph was a handful more. The cluster is fully redundant: data is triply replicated across nodes with automatic failover for monitors and managers. What took us several blog posts to automate and deploy took minutes to cluster.\nIf you want to practice it, you can leverage the destroy_lab.sh script, redeploy and master the commands.\nIn Part 5 we\u0026rsquo;ll deploy the Proxmox Backup Server and tie it into the cluster. See you then.\n","permalink":"http://localhost:1313/posts/vlab_part-4/","summary":"\u003ch1 id=\"virtual-proxmox-lab-part-4---clustering-corosync--ceph\"\u003eVirtual Proxmox Lab (Part 4) - Clustering: Corosync \u0026amp; Ceph\u003c/h1\u003e\n\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003cp\u003eLast week I did some work on the blog\u0026rsquo;s design, and I was a bit under the water, so I ended up forfeiting working on this blog post. Today I will start by doing a quick recap on how to get the lab deployed by using my GitHub files. It assumes that you have the Proxmox automated install \u003ccode\u003e.iso\u003c/code\u003e per Part 1. Then I will proceed to explain the CLI procedures for Corosync and Ceph. This part is way shorter and much more straightforward than what we\u0026rsquo;ve been doing before. I was aiming to get you to experience the process of scripting and figuring out what to do. Now we\u0026rsquo;re more practical. I don\u0026rsquo;t know if at any point I will script this. It\u0026rsquo;s possible. Maybe partially. You have a freedom to make mistakes and redeploy the whole lab in 3 minutes, use it.\u003c/p\u003e","title":"Virtual Proxmox Lab (Part 4) - Clustering: Corosync \u0026 Ceph"},{"content":"1. Introduction 1.1 Foreword Over the last 2 weeks since I have started to write this series, I was asked a few times \u0026ldquo;what about\u0026rdquo; Ansible, Terraform and other automation tools. The easy answer is: they are besides the point of what I\u0026rsquo;m trying to achieve right now. When I learned to build a VMware cluster, I learned to build the virtualization platform first, and to understand the network topology and the whys. I think that good teaching builds toward something. This tutorial series is a lot about learning a builder\u0026rsquo;s mindset and learning how to put some things together. It\u0026rsquo;s not about a quick recipe for a ready-to-go thing. If I was aiming at just that, I could easily go to a \u0026ldquo;here\u0026rsquo;s the GitHub repo, here are the requirements and some basic instructions and off you go\u0026rdquo;. Or even \u0026ldquo;here\u0026rsquo;s the repo, go to your chatbot and figure it out\u0026rdquo;. Instead, I want the reader to see the struggles and feel the needs for certain tools. I\u0026rsquo;m also adding a lot of personal touch, like the way I\u0026rsquo;m doing the ~/libvirt directory. I like the idea of taking the chance to teach a new trick or two, or go into territory that most DevOps Engineers or SREs don\u0026rsquo;t go to. If you think about it, in this series I\u0026rsquo;m teaching how to build a virtualization cluster. I\u0026rsquo;m also excited to iterate on my network diagram designs, on the cluster design itself and for people to maybe see it and feel stoked to get into \u0026ldquo;learning the craft\u0026rdquo; through repetition and persistence.\n1.2 What We Are Building That all being said, today we will be doing a little bit of automation. I will be teaching you how to write a couple of shell scripts to just put together and dismantle the whole virtual lab very easily, because this is the playground where we will be doing other interesting stuff in the future. I hope that this is not going in the direction of feature creep: when we are done with this series (and it looks like it will be more than 5 parts by now) we will have, in addition to the 3 virtual Proxmox nodes:\nA PBS VM on the same layer as the 3 PVE nodes. A PDM VM nested under the 3 PVE nodes. A Prometheus/Grafana dashboard monitoring the cluster. A Wazzuh VM doing the SIEM. All of these things, before they things which can be automated, are things which can be designed, installed, perfected. Personally, I don\u0026rsquo;t learn really well if I\u0026rsquo;m just copying ready-to-go recipes. I learn well by following along thoughtfully. If you enjoy that too, then we will have fun together. If not, maybe you\u0026rsquo;re looking at the wrong place, the internet is packed full of \u0026ldquo;ready-to-go\u0026rdquo; and AI can just as easily give you that.\n1.3 Back to the PVE Node\u0026rsquo;s Specs In Part 2 I provided the following table about the specs for the PVE nodes:\nVM Name Role vCPU RAM Boot Disk Storage Disks Purpose pve-01 PVE Node 1 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs pve-02 PVE Node 2 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs pve-03 PVE Node 3 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs And we went ahead and deployed pve-01 following these specifications. The vCPU should be of type host-passthrough and the disk controllers should be SCSI (virtio), which is not the same as just virtio. There are enough disks to build a fully redundant Ceph pool, allowing us to have a full node failure or single disk failures, this is the kind of resilience which is reasonable for a cluster of this size.\nAt the end of this part you will have a couple of scripts which will have the whole playground set up from scratch, identical and repeatable:\ncreate_iso.sh with this we will simplify the ISO creation process. deploy_lab.sh with this we will deploy the entire lab with the networks and the 3 nodes fully installed and network-configured. destroy_lab.sh with this we will destroy the whole lab: the 3 nodes, their storage and the networks, so that your host machine is clean. We will also write a handful of functions to facilitate the whole process. This will be the foundation for what comes next with building a cluster and adding PBS and PDM and will give us the chance to completely wreck things and retry them for as many times as we need.\n2. Writing the create_iso.sh script Back in Part 2 we already have the proto-script, which is our starting point:\nISO_FILE=\u0026#34;/var/lib/libvirt/images/proxmox-ve_9.1-1.iso\u0026#34; ANSWER_FILE=\u0026#34;guests/pve-01/answer_pve-01.toml\u0026#34; FIRST_BOOT=\u0026#34;guests/pve-01/firstboot_pve-01.sh\u0026#34; OUTPUT=\u0026#34;/var/lib/libvirt/images/pve-01_automated.iso\u0026#34; proxmox-auto-install-assistant prepare-iso \u0026#34;${ISO_FILE}\u0026#34; \\ --fetch-from iso \\ --answer-file \u0026#34;${ANSWER_FILE}\u0026#34; \\ --on-first-boot \u0026#34;${FIRST_BOOT}\u0026#34; \\ --output \u0026#34;${OUTPUT}\u0026#34; You will be writing guests/create_iso.sh.\n2.1 From Commands to Script Those four lines that we ran work, but they are not robust. If you run them from the wrong directory, forget which node you\u0026rsquo;re working with, or try to run them in a system where the proxmox-auto-install-assistant tool is not installed, it won\u0026rsquo;t work and you\u0026rsquo;ll be left guessing why. Besides that, it will require a bunch of copy and paste and editing the variables (or the command itself) every time you want to run it. We will be making this task easier and repeatable by adding:\nA proper header with usage documentation. Strict mode to catch errors early. Input validation for the node argument. Environment checks for OS and dependencies. Clear error messages that tell you what went wrong and how to fix it. 2.2 The Header and Strict Mode The proper shebang is #!/usr/bin/env bash which ensures that the script runs with bash regardless of where it\u0026rsquo;s installed on the system.\nThe set line enables bash\u0026rsquo;s strict mode:\n-e exits immediately if any command returns a non-zero status. -u treats unset variables as an error. If you typo a variable name, the script stops. -o pipefail makes sure that a pipeline | will return the exit status of the last command that failed, not just the final command. 2.3 Variables and Arguments NODE=\u0026#34;${1:-}\u0026#34; ISO_FILE=\u0026#34;/var/lib/libvirt/images/proxmox-ve_9.1-1.iso\u0026#34; ANSWER_FILE=\u0026#34;${NODE}/answer_${NODE}.toml\u0026#34; FIRST_BOOT=\u0026#34;${NODE}/firstboot_${NODE}.sh\u0026#34; OUTPUT=\u0026#34;/var/lib/libvirt/images/${NODE}_automated.iso\u0026#34; We\u0026rsquo;re pulling the node name from the first positional argument ${1}. The ${1:-} syntax provides an empty default if no argument is given, which prevents the -u flag from killing the script before we can show a helpful message.\nWe are writing the paths relative to /var/lib/libvirt as the default installation of libvirt stipulates. You could use here ${HOME}/libvirt instead, if you\u0026rsquo;re following along the bind mounts step laid out in Part 1. I\u0026rsquo;m assuming that you will run the script as root or have dealt with the directory permissions like I did.\nLet\u0026rsquo;s remember our directory structure from Part 1:\n$ tree -L 2 . ├── diagrams ├── guests │ ├── pbs │ ├── pve-01 │ ├── pve-02 │ └── pve-03 ├── host │ ├── configs │ │ ├── ceph-br.xml │ │ ├── default.xml │ │ ├── ha-br.xml │ │ ├── st-br.xml │ │ └── vm-br.xml │ └── scripts Now, if you remember part 2, we wrote file files answer_pve-01.toml and firstboot_pve-01.sh under guests/pve-01. We will have each virtual PVE node in their own folder and the script will look for those files according to the node\u0026rsquo;s name using ${ANSWER_FILE} and ${FIRST_BOOT}.\n2.4 Input Validation if [[ -z \u0026#34;${NODE}\u0026#34; ]] then echo \u0026#34;Usage: ${0} [pve-NN]\u0026#34; \u0026gt;\u0026amp;2 echo \u0026#34;Missing a pve node as argument.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi Before the script starts doing anything meaningful, we must check that the user actually provided a node name with the -z test verifying that the string is not empty.\nThings to take note here:\nRedirect error messages to STDERR with \u0026gt;\u0026amp;2 rather than STDOUT. It\u0026rsquo;s helpful when someone is piping the outputs and don\u0026rsquo;t want error messages and regular output mixed. Usage message showing how to invoke the script correctly is a stable best-practice. Exit with status 1 indicating failure, can be called with ${?}. 2.5 Environment Checks if ! grep -qi \u0026#39;trixie\u0026#39; /etc/os-release then echo \u0026#34;ERROR: Requires Debian 13 (Trixie).\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi if ! dpkg-query -W proxmox-auto-install-assistant \u0026amp;\u0026gt;/dev/null then echo \u0026#34;ERROR: Missing proxmox-auto-install-assistant package.\u0026#34; \u0026gt;\u0026amp;2 echo \u0026#34;Please refer to https://pve.proxmox.com/wiki/Automated_Installation for more information.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi The proxmox-auto-install-assistant only exists in Promox\u0026rsquo;s repos, which are only available on Debian (the base distro where Proxmox runs). For the version which we are working with right now, Debian 13 (Trixie) is required. If you\u0026rsquo;re following along on Arch, we will manage this by running Debian 13 inside a Distrobox container. Alternatively, you could be doing this on a computer running Debian 13 or Proxmox. I suspect it\u0026rsquo;s not far-fetched that it would also work on Ubuntu.\nThe first check grep /etc/os-release for \u0026ldquo;trixie\u0026rdquo; (case-insensitive). The second check with dpkg-query verifies that the required package is installed. We don\u0026rsquo;t need to see the output, so we redirect both STOUT and STDERR to the black hole with \u0026amp;\u0026gt;/dev/null. We care only for the exit status. We are checking both cases with ! meaning that this is a NOT conditional check.\n2.6 The Main Event proxmox-auto-install-assistant prepare-iso \u0026#34;${ISO_FILE}\u0026#34; \\ --fetch-from iso \\ --answer-file \u0026#34;${ANSWER_FILE}\u0026#34; \\ --on-first-boot \u0026#34;${FIRST_BOOT}\u0026#34; \\ --output \u0026#34;${OUTPUT}\u0026#34; This is the same command that we ran in Part 2, just with the variables substituted. I broke it in different lines with \\ for readability.\n2.7 Exit Status Handling EXIT_STATUS=\u0026#34;${?}\u0026#34; if [[ \u0026#34;${EXIT_STATUS}\u0026#34; -ne 0 ]] then echo \u0026#34;The script did not execute successfully!\u0026#34; \u0026gt;\u0026amp;2 exit \u0026#34;${EXIT_STATUS}\u0026#34; fi exit 0 The special variable ${?} holds the exit status of the last command. It\u0026rsquo;s a good idea to capture that immediately to avoid the possibility that any other command executed successfully will overwrite it. We are intently announcing that the script did not run successfully if the main event did not complete. We then end the script with exit 0 to make it clear that getting there is the definitive ending.\n2.8 A Word on the License Header You\u0026rsquo;ll notice that I added a GPLv3 license block at the top. I\u0026rsquo;m releasing these scripts under the GPL, because I want you and anyone else to be able to use, modify and share them freely, with the condition that your derivative works remain open. Picking the right license matters, and in this day and age in which corporations will take and not really give back, it matters even more. I\u0026rsquo;m a firm believer in the GPLv3 and also everything the Free Software Foundation stands for, and I encourage you to inform yourself about it. I might write a post on this topic later.\nThis is the final result:\n#!/usr/bin/env bash # # Script: create_iso.sh # Purpose: Simple script to make the process of creating automated PVE installation ISO portable # # Copyright (C) 2026 Thomas Lutkus # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Compatibility: Debian 13 (must run inside Distrobox on Arch Linux) # Dependencies: proxmox-auto-install-assistant # https://pve.proxmox.com/wiki/Automated_Installation#Assistant_Tool # Requires: root or full access to the libvirt directory # # Usage: ./create_iso.sh [PVE_NODE] # # Author: Thomas Lutkus # Date: 2026-01-22 # Version: 1.2 set -euo pipefail # Script constants and variables go here NODE=\u0026#34;${1:-}\u0026#34; ISO_FILE=\u0026#34;/var/lib/libvirt/images/proxmox-ve_9.1-1.iso\u0026#34; ANSWER_FILE=\u0026#34;${NODE}/answer_${NODE}.toml\u0026#34; FIRST_BOOT=\u0026#34;${NODE}/firstboot_${NODE}.sh\u0026#34; OUTPUT=\u0026#34;/var/lib/libvirt/images/${NODE}_automated.iso\u0026#34; # Verifications to run the script # Check usage if [[ -z \u0026#34;${NODE}\u0026#34; ]] then echo \u0026#34;Usage: ${0} [pve-NN]\u0026#34; \u0026gt;\u0026amp;2 echo \u0026#34;Missing a pve node as argument.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi # Check for super user privileges # if [[ \u0026#34;${UID}\u0026#34; -ne 0 ]] # then # echo \u0026#34;ERROR: Must run as root.\u0026#34; \u0026gt;\u0026amp;2 # exit 1 # fi # Check for Debian 13 if ! grep -qi \u0026#39;trixie\u0026#39; /etc/os-release then echo \u0026#34;ERROR: Requires Debian 13 (Trixie).\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi # Check for the assistant tool if ! dpkg-query -W proxmox-auto-install-assistant \u0026amp;\u0026gt;/dev/null then echo \u0026#34;ERROR: Missing proxmox-auto-install-assistant package.\u0026#34; \u0026gt;\u0026amp;2 echo \u0026#34;Please refer to https://pve.proxmox.com/wiki/Automated_Installation for more information.\u0026#34; \u0026gt;\u0026amp;2 exit 1 fi proxmox-auto-install-assistant prepare-iso \u0026#34;${ISO_FILE}\u0026#34; \\ --fetch-from iso \\ --answer-file \u0026#34;${ANSWER_FILE}\u0026#34; \\ --on-first-boot \u0026#34;${FIRST_BOOT}\u0026#34; \\ --output \u0026#34;${OUTPUT}\u0026#34; EXIT_STATUS=\u0026#34;${?}\u0026#34; if [[ \u0026#34;${EXIT_STATUS}\u0026#34; -ne 0 ]] then echo \u0026#34;The script did not execute successfully!\u0026#34; \u0026gt;\u0026amp;2 exit \u0026#34;${EXIT_STATUS}\u0026#34; fi exit 0 3. Writing the deploy_lab.sh Script 3.1 What it Does We already went through some concepts I adhere to when shell scripting when writing create_iso.sh, so I will not repeat myself on those. Instead, I will present some new things in this script which I find useful.\nThe script will create all the networks, provision the VMs and boot them. It will do some basic checks for the requirements and existence of files, and will also use some cosmetics for the output, so that it looks very professional. I find it encouraging when the script turns out like this one did.\n3.2 The Flow Nothing like a flowchart to show how it works. Since I\u0026rsquo;m flexing my drawing muscles whenever I can, I thought this script was a nice opportunity to throw a flowchart into our project:\nThis is by no means a software development project, but I think that it\u0026rsquo;s a nice visual reference for the code. I won\u0026rsquo;t repeat myself on shell scripting concepts, but I will explain some of the interesting stuff that it does.\n3.3 What\u0026rsquo;s New Here Status Functions and Cosmetics Why bother? Well, when deploying a bunch of VMs and networks a wall of white output text with barely any formatting doesn\u0026rsquo;t do wonders for identifying what is actually happening. Personally, I like neat, informative output. Color-coded output allows us to scan and identify problems instantly.\n# Status Colors RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[0;33m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color # Status functions ok() { echo -e \u0026#34;${GREEN}[OK]${NC} $1\u0026#34;; } info() { echo -e \u0026#34;${BLUE}[INFO]${NC} $1\u0026#34;; } warn() { echo -e \u0026#34;${YELLOW}[WARN]${NC} $1\u0026#34;; } fail() { echo -e \u0026#34;${RED}[FAIL]${NC} $1\u0026#34;; } status() { local label=\u0026#34;$1\u0026#34; # First arg: what we\u0026#39;re reporting on, e.g., \u0026#34;[VM] pve-01\u0026#34; local status=\u0026#34;$2\u0026#34; # Second arg: the status text, e.g., \u0026#34;OK\u0026#34; or \u0026#34;FAIL\u0026#34; local color=\u0026#34;$3\u0026#34; # Third arg: color variable, e.g., \u0026#34;${GREEN}\u0026#34; local width=50 # Fixed width for alignment local dots=$(( width - ${#label} )) # Calculate padding: 50 minus label length printf \u0026#34;%s %s %b%s%b\\n\u0026#34; \u0026#34;$label\u0026#34; \u0026#34;$(printf \u0026#39;.%.0s\u0026#39; $(seq 1 $dots))\u0026#34; \u0026#34;$color\u0026#34; \u0026#34;$status\u0026#34; \u0026#34;$NC\u0026#34; } header() { echo -e \u0026#34;${BLUE}\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo \u0026#34; Virtual Proxmox Lab Deployment\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo -e \u0026#34;${NC}\u0026#34; } footer() { echo -e \u0026#34;${GREEN}\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo \u0026#34; Deployment complete. VMs installing.\u0026#34; echo \u0026#34; Run: virsh start pve-{01,02,03} after install completes\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo -e \u0026#34;${NC}\u0026#34; } The header() and footer() functions are pretty self-evident. The other parts are as follows:\nStatus colors are just assigning names to the color codes so that they are easier to call.\nStatus functions are each for different purposes:\ninfo() is before configuring each network. ok() is when a network was defined/started successfully. warn() is when a network already existed/running. fail() is when there are missing config files (validation). status() is for the steps in the VM provisioning loop. When you look a the whole script and check it out with the help of flowchart it will be clearer how every piece comes together.\nParsing the VM Configuration File First you will create the file host/configs/vm.conf and will add the following content into it:\npve-01,02:00:00:00:01:01,02:00:00:01:01:01,02:00:00:FD:01:01,02:00:00:FE:01:01,02:00:00:FF:01:01 pve-02,02:00:00:00:01:02,02:00:00:01:01:02,02:00:00:FD:01:02,02:00:00:FE:01:02,02:00:00:FF:01:02 pve-03,02:00:00:00:01:03,02:00:00:01:01:03,02:00:00:FD:01:03,02:00:00:FE:01:03,02:00:00:FF:01:03 The new line at the end might be necessary, though the script has corrective measure for it. The structure is that of a CSV file with the following variable mapping on the script:\nVariable Field Maps to Purpose VM 1 --name VM name (pve-01, pve-02, pve-03) MAC_WAN 2 bridge=virbr0 Management/WAN network MAC_VM 3 bridge=vm-br VM traffic network MAC_HA 4 bridge=ha-br High availability/Corosync MAC_CEPH 5 bridge=ceph-br Ceph cluster traffic MAC_STORE 6 bridge=st-br Storage network We made a conscious choice for explicit MACs to have predictable addressing. That makes it easy for the auto installation of Proxmox and the first-boot script to configure the networking properly. We want to achieve proper networking before we would configure anything else in the cluster, that\u0026rsquo;s the desirable state when the installation is finished.\nThe core of the VM provisioning loop is right here:\nwhile IFS=\u0026#39;,\u0026#39; read -r VM MAC_WAN MAC_VM MAC_HA MAC_CEPH MAC_STORE || [[ -n \u0026#34;$VM\u0026#34; ]]; do ... done \u0026lt; \u0026#34;${VM_FILE} This is scanning the vm.conf file and getting each variable according to each line/node. The || [[ -n \u0026quot;$VM\u0026quot; ]] ending catches (or is supposed to) by keeping looping while read succeeds, OR while VM is non-empty. Otherwise, the while loop will end on the last line, before it\u0026rsquo;s processed.\nThe Complete deploy_lab.sh Script #!/usr/bin/env bash # # Script: deploy_lab.sh # Purpose: Script to deploy the entire Virtual Proxmox Lab # # Copyright (C) 2026 Thomas Lutkus # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Compatibility: distro-agnostic # Dependencies: libvirt, virt-install # Requires: qemu:///system access (root or libvirt group) # # Usage: ./deploy_lab.sh [VM_FILE] # # Author: Thomas Lutkus # Date: 2026-01-23 # Version: 1.0 set -euo pipefail # Script constants and variables go here # Status Colors RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[0;33m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color # Status functions ok() { echo -e \u0026#34;${GREEN}[OK]${NC} $1\u0026#34;; } info() { echo -e \u0026#34;${BLUE}[INFO]${NC} $1\u0026#34;; } warn() { echo -e \u0026#34;${YELLOW}[WARN]${NC} $1\u0026#34;; } fail() { echo -e \u0026#34;${RED}[FAIL]${NC} $1\u0026#34;; } status() { local label=\u0026#34;$1\u0026#34; local status=\u0026#34;$2\u0026#34; local color=\u0026#34;$3\u0026#34; local width=50 local dots=$(( width - ${#label} )) printf \u0026#34;%s %s %b%s%b\\n\u0026#34; \u0026#34;$label\u0026#34; \u0026#34;$(printf \u0026#39;.%.0s\u0026#39; $(seq 1 $dots))\u0026#34; \u0026#34;$color\u0026#34; \u0026#34;$status\u0026#34; \u0026#34;$NC\u0026#34; } header() { echo -e \u0026#34;${BLUE}\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo \u0026#34; Virtual Proxmox Lab Deployment\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo -e \u0026#34;${NC}\u0026#34; } footer() { echo -e \u0026#34;${GREEN}\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo \u0026#34; Deployment complete. VMs installing.\u0026#34; echo \u0026#34; Run: virsh start pve-{01,02,03} after install completes\u0026#34; echo \u0026#34;═══════════════════════════════════════════════════════════\u0026#34; echo -e \u0026#34;${NC}\u0026#34; } # Functional variables CONF_DIR=\u0026#34;../configs\u0026#34; VM_FILE=\u0026#34;${1:-$CONF_DIR/vm.conf}\u0026#34; declare -A NETWORKS NETWORKS=( [default]=\u0026#34;default.xml\u0026#34; [ceph-br]=\u0026#34;ceph-br.xml\u0026#34; [ha-br]=\u0026#34;ha-br.xml\u0026#34; [st-br]=\u0026#34;st-br.xml\u0026#34; [vm-br]=\u0026#34;vm-br.xml\u0026#34; ) header # Validation checks # Check for the VM_FILE [[ -f \u0026#34;${VM_FILE}\u0026#34; ]] || { fail \u0026#34;Missing VM file: ${VM_FILE}\u0026#34; \u0026gt;\u0026amp;2; exit 1; } # Check for the network configuration XML files for NET in \u0026#34;${!NETWORKS[@]}\u0026#34;; do FILE=\u0026#34;${CONF_DIR}/${NETWORKS[$NET]}\u0026#34; [[ -f \u0026#34;${FILE}\u0026#34; ]] || { fail \u0026#34;Missing network file: ${FILE}\u0026#34; \u0026gt;\u0026amp;2; exit 1; } done # Define the networks for NET in \u0026#34;${!NETWORKS[@]}\u0026#34;; do info \u0026#34;Configuring: ${NET}\u0026#34; FILE=\u0026#34;${CONF_DIR}/${NETWORKS[$NET]}\u0026#34; # Define the Network if virsh net-define \u0026#34;${FILE}\u0026#34; 2\u0026gt;/dev/null; then ok \u0026#34;Network ${NET} defined\u0026#34; else warn \u0026#34;Network ${NET} was already defined\u0026#34; fi # Start the network if virsh net-start \u0026#34;${NET}\u0026#34; 2\u0026gt;/dev/null; then ok \u0026#34;Network ${NET} started\u0026#34; else warn \u0026#34;Network ${NET} already running\u0026#34; fi # Autostart the network virsh net-autostart \u0026#34;${NET}\u0026#34; 2\u0026gt;/dev/null || true done # Set the VMs up with the attributes from the VM_FILE while IFS=\u0026#39;,\u0026#39; read -r VM MAC_WAN MAC_VM MAC_HA MAC_CEPH MAC_STORE || [[ -n \u0026#34;$VM\u0026#34; ]]; do echo \u0026#34;Deploying: ${VM}\u0026#34; # VM Configuration Attributes VM_RAM=20480 # 20GiB VM_CPU=4 ISO_PATH=\u0026#34;/var/lib/libvirt/images/${VM}_automated.iso\u0026#34; IMG_DIR=\u0026#34;/var/lib/libvirt/images\u0026#34; # Check for the ISO file [[ -f \u0026#34;${ISO_PATH}\u0026#34; ]] || { echo \u0026#34;Missing ISO: ${ISO_PATH}\u0026#34; \u0026gt;\u0026amp;2; exit 1; } # Deploy the VM status \u0026#34;[VM] ${VM}\u0026#34; \u0026#34;DEPLOYING\u0026#34; \u0026#34;${YELLOW}\u0026#34; if virt-install \\ --name \u0026#34;${VM}\u0026#34; \\ --ram \u0026#34;${VM_RAM}\u0026#34; \\ --vcpus \u0026#34;${VM_CPU}\u0026#34; \\ --cpu host-passthrough \\ --os-variant debian13 \\ --graphics vnc,listen=0.0.0.0 \\ --noautoconsole \\ --boot cdrom,hd \\ --cdrom \u0026#34;${ISO_PATH}\u0026#34; \\ --controller type=scsi,model=virtio-scsi \\ --disk path=\u0026#34;${IMG_DIR}/${VM}_root.qcow2\u0026#34;,size=32,format=qcow2,bus=scsi,cache=none,io=native \\ --disk path=\u0026#34;${IMG_DIR}/${VM}_osd1.qcow2\u0026#34;,size=150,format=qcow2,bus=scsi,cache=none,io=native \\ --disk path=\u0026#34;${IMG_DIR}/${VM}_osd2.qcow2\u0026#34;,size=150,format=qcow2,bus=scsi,cache=none,io=native \\ --network bridge=virbr0,model=virtio,mac=\u0026#34;${MAC_WAN}\u0026#34; \\ --network bridge=vm-br,model=virtio,mac=\u0026#34;${MAC_VM}\u0026#34; \\ --network bridge=ha-br,model=virtio,mac=\u0026#34;${MAC_HA}\u0026#34; \\ --network bridge=ceph-br,model=virtio,mac=\u0026#34;${MAC_CEPH}\u0026#34; \\ --network bridge=st-br,model=virtio,mac=\u0026#34;${MAC_STORE}\u0026#34; \u0026amp;\u0026gt;/dev/null; then status \u0026#34;[VM] ${VM}\u0026#34; \u0026#34;OK\u0026#34; \u0026#34;${GREEN}\u0026#34; else status \u0026#34;[VM] ${VM}\u0026#34; \u0026#34;FAIL\u0026#34; \u0026#34;${RED}\u0026#34; fi done \u0026lt; \u0026#34;${VM_FILE}\u0026#34; footer exit 0 4. Writing the destroy_lab.sh Script 4.1 What It Does This script reverses everything deploy_lab.sh created. VMs get shut down and undefined, their disks deleted, and the networks torn down. Run it when you want a clean slate to start over.\n4.2 Teardown Order and Idempotency Order matters: VMs first, then networks. You can\u0026rsquo;t undefine a network that still has VMs attached to it—libvirt will complain. This is the reverse of deployment, where we created networks before VMs.\nEvery destructive command has || true appended. This makes the script idempotent: run it once, run it five times, you get the same result with no errors. VM already destroyed? Fine, move on. Network already undefined? No problem. This is defensive scripting—you don\u0026rsquo;t want a teardown script to fail halfway because something was already partially cleaned up from a previous attempt.\nThe --remove-all-storage flag on virsh undefine deletes the qcow2 disk images along with the VM definition. No orphaned 150GB files cluttering your image directory.\n4.3 The Optional -i Flag By default, the script leaves your custom ISOs intact. Regenerating them requires the Debian 13 Distrobox container and takes time. If you\u0026rsquo;re iterating on the lab and just want to rebuild VMs, keeping the ISOs saves a step. Pass -i when you want a truly clean slate.\n4.4 The Script #!/usr/bin/env bash # # Script: destroy_lab.sh # Purpose: Script to tear down the whole Virtual Proxmox Lab easily. # # Copyright (C) 2026 Thomas Lutkus # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Dependencies: libvirt # Requires: qemu:///system access (root or libvirt group) # # Usage: ./destroy_lab.sh [-i] [CONFIG] # # Author: Thomas Lutkus # Date: 2026-01-23 # Version: 1.0 set -euo pipefail # Script constants and variables go here CONF_DIR=\u0026#34;../configs\u0026#34; IMG_DIR=\u0026#34;/var/lib/libvirt/images\u0026#34; DEL_ISO=\u0026#34;false\u0026#34; NETWORKS=(ceph-br ha-br st-br vm-br) # Functions usage() { echo \u0026#34;Usage: ${0} [-i]\u0026#34; \u0026gt;\u0026amp;2 echo \u0026#34; -i Also delete the custom ISO files\u0026#34; \u0026gt;\u0026amp;2 exit 1 } # Process script options while getopts \u0026#34;i\u0026#34; OPTION do case $OPTION in i) DEL_ISO=\u0026#34;true\u0026#34; ;; *) usage ;; esac done # Shift the option away shift $((OPTIND - 1)) # Assign a VM file file VM_FILE=\u0026#34;${1:-$CONF_DIR/vm.conf}\u0026#34; # Script execution # Tear down VMs first (before networks) while IFS=\u0026#39;,\u0026#39; read -r VM _; do echo \u0026#34;Removing: ${VM}\u0026#34; # Shutdown and delete the VM and all files virsh destroy \u0026#34;${VM}\u0026#34; 2\u0026gt;/dev/null || true virsh undefine \u0026#34;${VM}\u0026#34; --remove-all-storage 2\u0026gt;/dev/null || true # Delete the ISO if the option was selected if [[ \u0026#34;${DEL_ISO}\u0026#34; == \u0026#34;true\u0026#34; ]] then IMG_FILE=\u0026#34;${IMG_DIR}/${VM}_automated.iso\u0026#34; echo \u0026#34;Deleting the ISO ${IMG_FILE}\u0026#34; rm -f \u0026#34;${IMG_FILE}\u0026#34; fi done \u0026lt; \u0026#34;${VM_FILE}\u0026#34; # Shutdown and delete the networks (after VMs are gone) for NET in \u0026#34;${NETWORKS[@]}\u0026#34;; do echo \u0026#34;Removing network ${NET}\u0026#34; virsh net-destroy \u0026#34;${NET}\u0026#34; 2\u0026gt;/dev/null || true virsh net-undefine \u0026#34;${NET}\u0026#34; 2\u0026gt;/dev/null || true done exit 0 Important: I did not declare default in the \u0026quot;${NETWORKS}\u0026quot; list, because I still use it after I dismantle the lab. It\u0026rsquo;s just like the default network which libvirt install originally. You could add that to the script if you like.\n5. Deploying and Destroying the Virtual Lab This is the simplest part. I will assume that you have a terminal capable of tabs or tiles, and that you have some understanding of the commands. At his point, you could clone my GitHub repository to make sure that you have the exact files, as you already understand what we built:\nmkdir -p ~/Projects \u0026amp;\u0026amp; git clone https://github.com/tomlutkus/virtual-proxmox-lab ~/Projects/virtual-proxmox-lab 5.1 Make the Files for the Other Nodes Your directory structure should be something like this, if you haven\u0026rsquo;t cloned my repository. This is also what\u0026rsquo;s in there:\n├── guests │ ├── create_iso.sh │ ├── pve-01 │ │ ├── answer_pve-01.toml │ │ └── firstboot_pve-01.sh │ ├── pve-02 │ │ ├── answer_pve-02.toml │ │ └── firstboot_pve-02.sh │ └── pve-03 │ ├── answer_pve-03.toml │ └── firstboot_pve-03.sh ├── host │ ├── configs │ │ ├── ceph-br.xml │ │ ├── default.xml │ │ ├── ha-br.xml │ │ ├── st-br.xml │ │ ├── vm-br.xml │ │ └── vm.conf │ └── scripts │ ├── deploy_lab.sh │ ├── deploy_pve-01.sh │ ├── deploy_pve-02.sh │ ├── deploy_pve-03.sh │ └── destroy_lab.sh If you want to do things manually and you haven\u0026rsquo;t done it yet on Part 2, you can copy the files answer_pve-01.toml and firstboot_pve-01.sh from guests/pve-01 into guests/pve-0{2..3} and work the files to have the correct names and MAC addresses. You need to be very attentive. You could use my files from the repo instead.\n5.2 Build the ISO files Pre-requisite: Having the Proxmox VE 9.1 ISO you can download from the official website here. Move the ISO file: mv ~/Downloads/proxmox-ve_9.1-1.iso /var/lib/libvirt/images/ (assuming you configured directory permissions, otherwise use sudo).\ndistrobox enter pve-tools cd ~/Projects/virtual-proxmox-lab/guests chmod +x create_iso.sh for N in pve-0{1..3}; do ./create_iso.sh \u0026quot;${N}\u0026quot;; done ls /var/lib/libvirt/images/*automated.iso → you should see the files pve-01_automated.iso, pve-02_automated.iso and pve-03_automated.iso. 5.3 Open a virt-viewer to Watch Lab Deploy In another terminal run this to keep virt-viewer windows in parallel for each instance:\nfor N in pve-0{1..3}; do virt-viewer -w -r \u0026#34;${N}\u0026#34; \u0026amp; done 5.4 Run the deploy_lab.sh Script From the same or another terminal run:\ncd ~/Projects/virtual-proxmox-lab/host/scripts chmod +x deploy_lab.sh ./deploy_lab.sh You will see the installation unfold for all the 3 nodes. When they power down, start them again:\nfor N in pve-0{1..3}; do virsh start \u0026#34;${N}\u0026#34;; done And wait for them to complete the boot processes while they run the first-boot script.\n5.5 Test that it Works, Destroy, Repeat If you followed the series faithfully until now, you should be able to:\nssh root@192.168.122.1 And log into the pve-01 lab Proxmox VM successfully. You might need to specify your SSH key, or enter the password Secret123! depending on how your computer is configured. Once in the prompt, try:\nping -c 4 192.168.122.2 ping -c 4 192.168.122.3 If both were successful, your lab is fully configured and networked.\nTo destroy it, just run:\nchmod +x destroy_lab.sh ./destroy_lab.sh And watch as it\u0026rsquo;s torn down and your workstation is completely clean of the VMs and networks.\n6. Final Words on Part 3 This one was a lot of work to write. I did not want to turn it into a shell script course, but I wanted instead for you to get a feeling of the process of building the lab topology manually, seeing what works, and then automating the process. This is a fully functional staging environment to practice Proxmox and Ceph clustering, which will be topics for the the upcoming parts in the series.\nMy goal is not to help you get there as fast as possible. It is, instead, to hold your hands as you go through the mental processes yourself. Maybe you will have better ideas for some parts, or will have more knowledge about some things and that\u0026rsquo;s great! I\u0026rsquo;d love to hear about that.\nI\u0026rsquo;m still working on the concept for the next session, which is coming next week. We will progress with the clustering and towards having a fully functional Proxmox infrastructure. Stay tuned!\nGod bless you!\n","permalink":"http://localhost:1313/posts/vlab_part-3/","summary":"\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003ch3 id=\"11-foreword\"\u003e1.1 Foreword\u003c/h3\u003e\n\u003cp\u003eOver the last 2 weeks since I have started to write this series, I was asked a few times \u0026ldquo;what about\u0026rdquo; Ansible, Terraform and other automation tools. The easy answer is: they are besides the point of what I\u0026rsquo;m trying to achieve right now. When I learned to build a VMware cluster, I learned to build the virtualization platform first, and to understand the network topology and the whys. I think that good teaching builds toward something. This tutorial series is a lot about learning a builder\u0026rsquo;s mindset and learning how to put some things together. It\u0026rsquo;s not about a quick recipe for a ready-to-go thing. If I was aiming at just that, I could easily go to a \u0026ldquo;here\u0026rsquo;s the GitHub repo, here are the requirements and some basic instructions and off you go\u0026rdquo;. Or even \u0026ldquo;here\u0026rsquo;s the repo, go to your chatbot and figure it out\u0026rdquo;. Instead, I want the reader to see the struggles and feel the needs for certain tools. I\u0026rsquo;m also adding a lot of personal touch, like the way I\u0026rsquo;m doing the \u003ccode\u003e~/libvirt\u003c/code\u003e directory. I like the idea of taking the chance to teach a new trick or two, or go into territory that most DevOps Engineers or SREs don\u0026rsquo;t go to. If you think about it, in this series I\u0026rsquo;m teaching how to build a virtualization cluster. I\u0026rsquo;m also excited to iterate on my network diagram designs, on the cluster design itself and for people to maybe see it and feel stoked to get into \u0026ldquo;learning the craft\u0026rdquo; through repetition and persistence.\u003c/p\u003e","title":"Virtual Proxmox Lab (Part 3) - Scripting the Lab Deployment"},{"content":"1. Introduction Now, we will be deploying an enterprise-grade lab. Therefore we need to think about how we do things in production: we can\u0026rsquo;t be clicking around and hoping for the best. This quickly compounds. We also need things to scale, to be idempotent (magic word for \u0026ldquo;always works the same\u0026rdquo;) and self-healing. As I see this series going right now, we will be iterating the design of our lab a little bit as we go, since there is always room for improving and learning. That being said, I have done some changes to Part 1 to keep things consistent with this new post, add some stuff and fix minor errors. You should check it out.\n1.1. Lab Design Progress You will notice that I introduced a file server in the new networking diagram. That will be useful later for our Proxmox Backup Server configuration. I also introduced some other VMs we will be configuring in this lab later in this series:\n1.2. IP and MAC Addresses This is the full table of the MAC and IP addresses we are going to be using. Notice that the vm network will not be directly accessible by the PVE cluster. Also notice that we are using 02:00:00 MAC addresses, which indicate a \u0026ldquo;local administered address\u0026rdquo; (LAA). That means we are specifying them as not a manufacturer-assigned address. Take a long look at the MAC addresses and the IP addresses to see that we are going with something that makes sense:\nNode Network IP Address MAC Address Alias pve-01 WAN 192.168.122.1 02:00:00:00:01:01 wan vm 02:00:00:01:01:01 vm HA/Corosync 10.0.253.1 02:00:00:FD:01:01 ha Ceph 10.0.254.1 02:00:00:FE:01:01 ceph Storage 10.0.255.1 02:00:00:FF:01:01 store pve-02 WAN 192.168.122.2 02:00:00:00:02:02 wan vm 02:00:00:01:02:02 vm HA/Corosync 10.0.253.2 02:00:00:FD:02:02 ha Ceph 10.0.254.2 02:00:00:FE:02:02 ceph Storage 10.0.255.2 02:00:00:FF:02:02 store pve-03 WAN 192.168.122.3 02:00:00:00:03:03 wan vm 02:00:00:01:03:03 vm HA/Corosync 10.0.253.3 02:00:00:FD:03:03 ha Ceph 10.0.254.3 02:00:00:FE:03:03 ceph Storage 10.0.255.3 02:00:00:FF:03:03 store pbs vm 10.0.1.10 02:00:00:01:00:0A vm Storage 10.0.255.10 02:00:00:FF:00:0A store 1.3. Storage Design and Allocation Rather than leaving everything to chance, let\u0026rsquo;s do it very tight and predictable, as you would like in a production environment. These are the specs for each of the VMs which are \u0026ldquo;virtual hosts\u0026rdquo; (the Proxmox nodes and the future storage server):\nVM Name Role vCPU RAM Boot Disk Storage Disks Purpose vm-pve-01 PVE Node 1 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs vm-pve-02 PVE Node 2 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs vm-pve-03 PVE Node 3 4 20GB 32GB (sda) 2x 150GB (sdb, sdc) Proxmox + Ceph OSDs pbs Storage 2 4GB 20GB 1x 500GB NFS/Shared Storage You could probably get away with 16GB RAM for the PVE nodes, however that will require some aggressive tuning of Ceph. Don\u0026rsquo;t expect to be able to test fail over if you go that route. I think that it will also work if the host system has 64GB RAM if you\u0026rsquo;re not going crazy with browsers and apps. Memory prices are a problem these days, but if you have 96GB RAM you\u0026rsquo;re in a better spot. You could still leverage a bunch of physical hardware, but you need a managed switch to have the network right, so there\u0026rsquo;s always a trade-off. This is how our finished storage will look like:\n2. Setting up the Auto-Installer We need to install the proxmox-auto-install-assistant program. Proxmox maintains it for Debian, because that\u0026rsquo;s the platform where Proxmox runs. Assuming that you are following along on Arch: we can easily fix this with Distrobox: run a lightweight Debian 13 container and manager it from there. You could get away with a regular docker container or a Debian 13 (or Proxmox) VM too. I\u0026rsquo;m covering the Arch Linux steps:\n2.1. Arch: Setting up Distrobox and Debian 13 First install it with pacman:\nsudo pacman -S distrobox Next, create a Debian 13 (at this time) container:\ndistrobox create -i debian:latest -n pve-tools We gave it the name pve-tools. Enter the container, it should take a couple of minutes the first time:\ndistrobox enter pve-tools Once you\u0026rsquo;re in, your prompt will probably look a bit different from when you were directly on Arch. Make sure that the packages are running on the latest version:\nsudo apt update \u0026amp;\u0026amp; sudo apt dist-upgrade -y Next, make sure the keyrings directory exists:\nsudo mkdir -p /etc/apt/keyrings Then, add the Proxmox GPG key:\nsudo wget https://enterprise.proxmox.com/debian/proxmox-release-trixie.gpg -O /etc/apt/keyrings/proxmox-release-trixie.gpg Add the Proxmox no-subscription repository:\necho \u0026#34;deb [signed-by=/etc/apt/keyrings/proxmox-release-trixie.gpg] http://download.proxmox.com/debian/pve trixie pve-no-subscription\u0026#34; | sudo tee /etc/apt/sources.list.d/pve.list Then, set the right permissions to the keyring directory, update again and install the proxmox-auto-install-assistant:\nsudo chmod 644 /etc/apt/keyrings/proxmox-release-trixie.gpg sudo apt update \u0026amp;\u0026amp; sudo apt install -y proxmox-auto-install-assistant Check that it\u0026rsquo;s installed:\nproxmox-auto-install-assistant --version For now, open another terminal (or terminal tab) or leave the Distrobox container by typing exit.\n2.2. Setting up the Auto-Installer ISO 2.2.1. Create New Files We will be exercising how to do this with node pve-01, and then we will replicate the steps to the other nodes later. Inside our project\u0026rsquo;s folder, we will be heading to the folder to deal with files related to the guest systems. From the project\u0026rsquo;s folder, run:\ntouch guests/pve-01/{answer_pve-01.toml,firstboot_pve-01.sh} Run tree -L 2 from the project\u0026rsquo;s folder and your folder should be looking something like this:\n$ tree -L 2 . ├── diagrams ├── guests │ ├── pbs │ ├── pve-01 │ │ ├── answer_pve-01.toml │ │ └── firstboot_pve-01.sh │ ├── pve-02 │ └── pve-03 ├── host │ ├── configs │ │ ├── ceph-br.xml │ │ ├── default.xml │ │ ├── ha-br.xml │ │ ├── st-br.xml │ │ └── vm-br.xml │ └── scripts The two new files we\u0026rsquo;ve created, and their function are as follows:\nanswer_pve-01.toml: The program proxmox-auto-install-assistant will use this file to configure the auto-installation of the Proxmox node. firstboot_pve-01.sh: The program proxmox-auto-install-assistant will insert this script to be executed in the first boot after the installation. We will use it to set a few extra configurations. 2.2.2. Working on the Answer File We would like to be using an ssh key-pair to connect to the servers when they are configured. If you don\u0026rsquo;t have one and don\u0026rsquo;t know how to generate one:\nssh-keygen -t ed25519 -C \u0026#34;yourname@machine\u0026#34; And then cat .ssh/id-ed25519.pub. That\u0026rsquo;s your public key. You will copy it into the answer configuration file.\nYou will edit your file with vim guests/pve-01/answer_pve-01.toml. This is my file, adjust yours according to your location, keyboard type and timezone and making sure to copy the SSH key between the double quotes:\n[global] keyboard = \u0026#34;en-us\u0026#34; country = \u0026#34;ie\u0026#34; fqdn = \u0026#34;pve-01.lab.local\u0026#34; mailto = \u0026#34;admin@lab.local\u0026#34; timezone = \u0026#34;Europe/Dublin\u0026#34; root-password = \u0026#34;Secret123!\u0026#34; root-ssh-keys = [ \u0026#34;ssh-ed25519 AAAAC3...\u0026#34; ] [network] source = \u0026#34;from-answer\u0026#34; cidr = \u0026#34;192.168.122.1/24\u0026#34; gateway = \u0026#34;192.168.122.254\u0026#34; dns = \u0026#34;1.1.1.1\u0026#34; [network.filter] ID_NET_NAME_MAC = \u0026#34;*020000000101\u0026#34; [network.interface-name-pinning] enabled = true [network.interface-name-pinning.mapping] \u0026#34;02:00:00:00:01:01\u0026#34; = \u0026#34;wan\u0026#34; \u0026#34;02:00:00:01:01:01\u0026#34; = \u0026#34;nic_vm\u0026#34; \u0026#34;02:00:00:fd:01:01\u0026#34; = \u0026#34;ha\u0026#34; \u0026#34;02:00:00:fe:01:01\u0026#34; = \u0026#34;ceph\u0026#34; \u0026#34;02:00:00:ff:01:01\u0026#34; = \u0026#34;store\u0026#34; [disk-setup] filesystem = \u0026#34;ext4\u0026#34; disk-list = [\u0026#34;sda\u0026#34;] [first-boot] source = \u0026#34;from-iso\u0026#34; ordering = \u0026#34;before-network\u0026#34; The trailing space might be important if you have issues, be sure to copy it too.\n2.2.3. Working on the First Boot You will now vim guests/pve-01/firstboot_pve-01.sh and copy the script bellow and save it. I will then explain some things:\n#!/bin/bash # 1. Disable enterprise repos, enable no-subscription rm -f /etc/apt/sources.list.d/pve-enterprise.list cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list.d/pve-no-subscription.sources Types: deb URIs: http://download.proxmox.com/debian/pve Suites: trixie Components: pve-no-subscription Signed-By: /usr/share/keyrings/proxmox-archive-keyring.gpg EOF cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list.d/ceph.sources Types: deb URIs: http://download.proxmox.com/debian/ceph-squid Suites: trixie Components: no-subscription Signed-By: /usr/share/keyrings/proxmox-archive-keyring.gpg EOF # 2. Hosts entries for cluster cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/hosts 192.168.122.1 pve-01.lab.local pve-01 192.168.122.2 pve-02.lab.local pve-02 192.168.122.3 pve-03.lab.local pve-03 EOF # 3. Network config cat \u0026lt;\u0026lt;EON \u0026gt; /etc/network/interfaces auto lo iface lo inet loopback auto wan iface wan inet static address 192.168.122.1/24 gateway 192.168.122.254 iface nic_vm inet manual auto vm_br iface vm_br inet manual bridge-ports nic_vm bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 10,20,30 # 10 = intranet, 20 = dmz, 30 = mgmt auto ha iface ha inet static address 10.0.253.1/24 auto ceph iface ceph inet static address 10.0.254.1/24 auto store iface store inet static address 10.0.255.1/24 EON reboot What we are doing here is simple:\nDisabling the Proxmox enterprise repositories and utilizing the no-subscription repositories. Adding the future Proxmox nodes to the /etc/hosts file so that this node knows them by name. Doing the final round of configuring this node\u0026rsquo;s network interfaces into their permanent configuration. 3. Auto-Installing Proxmox VE 3.1. Obtain the Proxmox VE 9.1 ISO All that we did so far was in preparation for this moment: we will now create the ISO file for the auto-install. As a requirement, you must download the Proxmox VE 9.1 ISO from the Proxmox website. Let\u0026rsquo;s use wget:\nsudo pacman -S wget wget --show-progress -O /var/lib/libvirt/images/proxmox-ve_9.1-1.iso https://enterprise.proxmox.com/iso/proxmox-ve_9.1-1.iso This should work if you followed section 2.1 from Part 1 of this tutorial series. If not, you need to run wget as sudo, or add the following ACLs to the /var/lib/libvirt directory. This is preferable as it will facilitate other steps in the future:\n# Apply rwx permissions for your user to all existing files/dirs recursively sudo setfacl -R -m u:$(whoami):rwx,m::rwx /var/lib/libvirt/ # Set default ACL - new files/dirs created inside inherit these permission sudo setfacl -R -d -m u:$(whoami):rwx,m::rwx /var/lib/libvirt/ Check that you have it:\n$ ls -l /var/lib/libvirt/images/proxmox-ve_9.1-1.iso # Expected output -rw-r--r-- 1 tlutkus tlutkus 17497854 Jan 25 15:36 /var/lib/libvirt/images/proxmox-ve_9.1-1.iso 3.2. Create the Auto-Install ISO for pve-01 Head back into your Distrobox instance distrobox enter pve-tools. From our project\u0026rsquo;s directory:\nISO_FILE=\u0026#34;/var/lib/libvirt/images/proxmox-ve_9.1-1.iso\u0026#34; ANSWER_FILE=\u0026#34;guests/pve-01/answer_pve-01.toml\u0026#34; FIRST_BOOT=\u0026#34;guests/pve-01/firstboot_pve-01.sh\u0026#34; OUTPUT=\u0026#34;/var/lib/libvirt/images/pve-01_automated.iso\u0026#34; proxmox-auto-install-assistant prepare-iso \u0026#34;${ISO_FILE}\u0026#34; \\ --fetch-from iso \\ --answer-file \u0026#34;${ANSWER_FILE}\u0026#34; \\ --on-first-boot \u0026#34;${FIRST_BOOT}\u0026#34; \\ --output \u0026#34;${OUTPUT}\u0026#34; Check that this step ran successfully with ls -l /var/lib/libvirt/images/pve-01_automated.iso:\n$ ls -l /var/lib/libvirt/images/pve-01_automated.iso # Expected Output -rw-r--r--+ 1 libvirt-qemu libvirt-qemu 1833369600 Jan 23 17:13 /var/lib/libvirt/images/pve-01_automated.iso You could exit Distrobox now, or move to another terminal window/tab.\n3.3. Auto-Install Proxmox on pve-01 This is it: we are going to deploy pve-01 automatically. for this step, I think that you will want to see the fireworks. It\u0026rsquo;s better if you have a couple of terminal windows/tabs available. From the first one do:\nvirt-viewer -r -w pve-01 A virt-viewer window will pop up. Move it to a side. Head into your second terminal window and type/paste the following command. Watch the Proxmox installation unfold on the virt-viewer window we just created. It will run all the way to the end and power off the VM:\n# Engineered Specs VM_NAME=\u0026#34;pve-01\u0026#34; VM_RAM=20480 # 20GiB - adjust to your case VM_CPU=4 ISO_PATH=\u0026#34;/var/lib/libvirt/images/pve-01_automated.iso\u0026#34; IMG_DIR=\u0026#34;/var/lib/libvirt/images\u0026#34; # Deployment virt-install \\ --name \u0026#34;$VM_NAME\u0026#34; \\ --ram \u0026#34;$VM_RAM\u0026#34; \\ --vcpus \u0026#34;$VM_CPU\u0026#34; \\ --cpu host-passthrough \\ --os-variant debian13 \\ --graphics vnc,listen=0.0.0.0 \\ --noautoconsole \\ --boot cdrom,hd \\ --cdrom \u0026#34;$ISO_PATH\u0026#34; \\ --controller type=scsi,model=virtio-scsi \\ --disk path=\u0026#34;$IMG_DIR/${VM_NAME}_root.qcow2\u0026#34;,size=32,format=qcow2,bus=scsi,cache=none,io=native \\ --disk path=\u0026#34;$IMG_DIR/${VM_NAME}_osd1.qcow2\u0026#34;,size=150,format=qcow2,bus=scsi,cache=none,io=native \\ --disk path=\u0026#34;$IMG_DIR/${VM_NAME}_osd2.qcow2\u0026#34;,size=150,format=qcow2,bus=scsi,cache=none,io=native \\ --network bridge=virbr0,model=virtio,mac=02:00:00:00:01:01 \\ --network bridge=vm-br,model=virtio,mac=02:00:00:01:01:01 \\ --network bridge=ha-br,model=virtio,mac=02:00:00:FD:01:01 \\ --network bridge=ceph-br,model=virtio,mac=02:00:00:FE:01:01 \\ --network bridge=st-br,model=virtio,mac=02:00:00:FF:01:01 Your virt-viewer screen should give you this, wait for it to run automatically:\nOnce Proxmox installed and powered off, we need to do the first boot, so that the firstboot_pve-01.sh script will run automatically. Run:\nvirsh start pve-01 It will cycle one or two times through the first boot. Wait for up to a minute (depending on your hardware specs) for it to give you a terminal:\nAt his point you know your pve-01 node has deployed successfully. Let\u0026rsquo;s make sure it\u0026rsquo;s really configured as we need it to, make sure that the firstboot_pve-01.sh script ran properly:\nssh -i .ssh/id-ed25519 root@192.168.122.1 Be mindful to specify the correct ssh key for your case. Once you\u0026rsquo;re in, run ip a and you should get a similar output:\nroot@pve-01:~$ ip a | grep -E \u0026#39;(wan|nic_vm|ha|ceph|store|vm_br)\u0026#39; --color=auto 2: wan: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 192.168.122.1/24 scope global wan 3: nic_vm: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel master vm_br state UP group default qlen 1000 4: ha: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 10.0.253.1/24 scope global ha 5: ceph: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 10.0.254.1/24 scope global ceph 6: store: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 inet 10.0.255.1/24 scope global store 7: vm_br: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 If you\u0026rsquo;re looking at the same output: congratulations, you just finished your auto-install of pve-01. You can exit the pve-01 shell. Additionally, virsh destroy pve-01 to power it off and clean up after yourself.\n4. Final Words for Part 2 We just did all the steps to get the first node pve-01 deployed. As a homework, before you head into Part 3 next week, you go go about reproducing what we just did for pve-01 for pve-02 and pve-03, being mindful to replace all the names and addresses in the files and commands we used. This would be a good exercise. That\u0026rsquo;s not what we are doing next week. Instead, we will be using shell scripts to automate everything we\u0026rsquo;ve done so far, and make it idempotent and repeatable. As it was with Part 1, I might work on improvements and make further changes to the diagram. Thank you for stick with me until the end. Stay tuned for more!\nGod bless you.\n","permalink":"http://localhost:1313/posts/vlab_part-2/","summary":"\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003cp\u003eNow, we will be deploying an enterprise-grade lab. Therefore we need to think about how we do things in production: we can\u0026rsquo;t be clicking around and hoping for the best. This quickly compounds. We also need things to scale, to be idempotent (magic word for \u0026ldquo;always works the same\u0026rdquo;) and self-healing. As I see this series going right now, we will be iterating the design of our lab a little bit as we go, since there is always room for improving and learning. That being said, I have done some changes to Part 1 to keep things consistent with this new post, add some stuff and fix minor errors. You should check it out.\u003c/p\u003e","title":"Virtual Proxmox Lab (Part 2) - Auto-Install Proxmox"},{"content":"1. Introduction 1.1. Lab Design I have some cool experience deploying and configuring a Proxmox cluster with High Availability and fully redundant Ceph in production. I thought it\u0026rsquo;d be cool to share my insights. Since I have been meaning to redeploy my lab on my laptop to test some things, I thought this is a good time. To start, this post will be about setting up the lab environment and install one Proxmox instance. I\u0026rsquo;m going to write a few other tutorials as I go. You are free to use the content here as you see fit. I would appreciate being credited, but that\u0026rsquo;s not a requirement.\nThis a drawing of a production-ready virtualization cluster I designed. Please note that this design works with whatever you prefer: Proxmox, XCP-ng, VMware, Linux. I\u0026rsquo;m using Proxmox because it\u0026rsquo;s very approachable and free.\nIt ticks all the boxes you\u0026rsquo;d need for a production environment:\nHigh Availability: 3 virtualization nodes capable of Corosync or whatever else you virtualization platform uses. You will notice that I deployed 2 virtual firewalls. With pfSense or opnsense you can also have the firewalls doing HA. Redundancy: The 3 virtualization nodes can have fully redundant Ceph. If you have enough NICs on the hosts and the switches, you can also have redundant network. The Hetzner vSwitch also offers a flexible solution if you prefer to approach this with just one firewall VM on HA: the IP will follow it seamlessly if it\u0026rsquo;s moved to a different virtualization node. Network segmentation: Separate physical networks for different purposes. Good for security, great for performance. A lot of Virtualization labs I see floating around in tutorials are built using a bunch of real hardware. While that\u0026rsquo;s undeniably cool and wholesome, we not always have the luxury of getting a bunch of extra computers. To be honest, nowadays I think it\u0026rsquo;s kind of wasteful in terms of energy, and also in terms of having idle PCs just for this if you end up powering them off. I think that Linux virtualization is mature enough that you can do really cool stuff with just one computer. In this series I will teach you to do precisely that.\n1.2. Requirements I\u0026rsquo;m not gonna lie: you need somewhat robust hardware for this. At this point if you\u0026rsquo;re not thinking about having a serious home lab to experiment with containers and virtualization, you might be in the wrong field. Serious. This is supposed to be enjoyable.\nI\u0026rsquo;m running a laptop with:\nRyzen 9 7940HS (8c/16t). 96GB DDR5 RAM (it\u0026rsquo;s crazy expensive to acquire RAM right now). 2TB WD SN850x NVMe (on top of my boot drive). You could probably get away with:\n6c/12t CPU (must be Intel VT-d or AMD-V). Check: lscpu | grep Virtualization. 64GB RAM. You will necessarily need to tune Ceph heavily. Don\u0026rsquo;t expect to fully test HA effectively (though I might be wrong). 1TB NVMe. You need an NVMe drive, or performance will suffer too much. Alternatively, you could have 3 used mini-PCs. Some old SFF (small form-factor) or NUC-style could work. In this case:\n4c: Having hyper-threading is nice-to-have, but I think you can live without it. 16GB RAM. More than 16GB RAM is actually better for what we are trying to achieve. Optimally 20GB+ per node. 250-300GB NVMe per node. Regular SSDs will work too. Network: 1x VLAN-capable managed switch. You need to be able to segregate the networks. 3 4-8 ports dumb switches will also work. Because you probably won\u0026rsquo;t have multiple NICs, you will need to improvise with USB NICs. Any 1Gbps models will do. Optimally 5 per node, but you can simplify. The physical hardware alternative is, in my opinion, clunky and can end up costing more than having a decent laptop or desktop, because the networking will require a lot of physical hardware. If you go that route, I leave it up to you.\nOur virtual lab is being set up on an Arch Linux host. I love Arch, you can do anything with it. It\u0026rsquo;s Linux the way it\u0026rsquo;s supposed to be. I don\u0026rsquo;t go crazy with ricing, I think it burns too much time and I already have to babysit GitHub projects and my work environment. Still, the AUR and Distrobox make many things much easier. You could adapt this guide easily to run on Debian-based or RHEL-based, I bet your favorite AI tool will have no problems to translate the steps into your favorite distro.\nThis is the lab we will be building, I think this is remarkably close to a production system:\n2. Setting Up the Environment 2.1. Setting up /var/lib/libvirt If you go with the defaults, you could skip to #2. I, however, have a matter of personal preference: I like to do things from my ~/ directory, and on my machine I have a 2TB NVMe specifically allocated for /home. I thought I\u0026rsquo;d go with symlinks, but I had some issues with this a while ago. I did some research and found out that bind mounts are the best approach: you get the same filesystem location exposed at two paths. Avoids all the potential weirdness from using symlinks for this. This is where all of your virtualization files, which consume a lot of disk space, will reside.\n# Create ~/libvirt mkdir /home/${USER}/libvirt # Create /var/lib/libvirt sudo mkdir /var/lib/libvirt # Add the entry to /etc/fstab echo \u0026#34;/home/${USER}/libvirt /var/lib/libvirt none bind 0 0\u0026#34; | sudo tee -a /etc/fstab # Reload so that you can apply systemctl daemon-reload # Mount it mount -a # Test it touch ~/libvirt/testfile ls /var/lib/libvirt/testfile My /home is encrypted on a separate volume as /. As long as the new mount is at the bottom of /etc/fstab, which it should, it should be handled automatically if you followed my steps.\nFor this to work well, we need to manage directory permissions with ACLs, so that all the necessary users can operate in the directory:\n# Ensure ~/ is traversable chmod 755 ~/ # Set base permissions on libvirt dir chmod 775 ~/libvirt # Install the ACL package sudo pacman -S acl # Apply rwx permissions for your user to all existing files/dirs recursively sudo setfacl -R -m u:$(whoami):rwx,m::rwx ~/libvirt/ # Set default ACL - new files/dirs created inside inherit these permission sudo setfacl -R -d -m u:$(whoami):rwx,m::rwx ~/libvirt/ There will be a bunch of permission steps later if you are doing this. In this guide I will be referring to the default /var/lib/libvirt directory so that it\u0026rsquo;s portable if you haven\u0026rsquo;t made the same choices.\n2.2. Configuring the Virtualization Suite 2.2.1. Setting up the System We will now install the necessary virtualization packages:\nsudo pacman -S libvirt qemu-full virt-manager virt-viewer dnsmasq ebtables dmidecode Once that\u0026rsquo;s done, add your user to the libvirt group:\nsudo usermod -aG libvirt $(whoami) Then enable and start the libvirtd service:\nsudo systemctl enable --now libvirtd And check if no errors with systemctl status libvirtd, it should show something like this:\n● libvirtd.service - libvirt legacy monolithic daemon Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; preset: disabled) Active: active (running) since Sun 2026-01-18 17:09:03 GMT; 5s ago Invocation: 43007c7f51c445a485af01f5f3ba08f5 TriggeredBy: ● libvirtd.socket ● libvirtd-admin.socket ● libvirtd-ro.socket Docs: man:libvirtd(8) https://libvirt.org/ Main PID: 11650 (libvirtd) Tasks: 21 (limit: 32768) Memory: 8.6M (peak: 10M) CPU: 695ms CGroup: /system.slice/libvirtd.service └─11650 /usr/bin/libvirtd --timeout 120 Jan 18 17:09:03 laptop systemd[1]: Starting libvirt legacy monolithic daemon... Jan 18 17:09:03 laptop systemd[1]: Started libvirt legacy monolithic daemon. Also check that the daemon is responding to commands with virsh list --all, it should return an empty table:\n$ virsh list --all # Expected output: Id Name State -------------------- Now you need to configure your user to connect to qemu:///system where the real networking happens, rather than qemu:///session:\nAdd this line to your .bashrc file and reload your shell:\necho \u0026#39;export LIBVIRT_DEFAULT_URI=\u0026#34;qemu:///system\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Test with virsh uri, if it shows qemu:///system, you\u0026rsquo;re good to go:\n$ virsh uri # Expected output: qemu:///system 2.2.2. Setting up the Project Directory We will be building a directory structure where we will be storing our files:\nmkdir -p ~/Projects/virtual-proxmox-lab/{diagrams,guests/{pve-01,pve-02,pve-03,pbs},host/{configs,scripts}} We might be changing things in this directory later on. This is how it should look like right now:\n$ tree -L 2 virtual-proxmox-lab/ # Expected output: virtual-proxmox-lab/ ├── diagrams ├── guests │ ├── pbs │ ├── pve-01 │ ├── pve-02 │ └── pve-03 ├── host │ ├── configs │ └── scripts When I refer to the \u0026ldquo;project directory\u0026rdquo; or to virtual-proxmox-lab this is where I expect you to go, regardless of where you created it in your system.\n2.3. Setting up the Networks 2.3.1. Redefine default First we will set up the default NAT network with the settings we desire. It\u0026rsquo;s likely already set up, so run:\nvirsh net-stop default virsh net-destroy default virsh net-undefine default Then go to your directory ~/Projects/virtual-proxmox-lab/host/configs. This is where we will leave template files. Once there vim default.xml and add this:\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;default\u0026lt;/name\u0026gt; \u0026lt;forward mode=\u0026#34;nat\u0026#34;/\u0026gt; \u0026lt;bridge name=\u0026#34;virbr0\u0026#34;/\u0026gt; \u0026lt;ip address=\u0026#34;192.168.122.254\u0026#34; netmask=\u0026#34;255.255.255.0\u0026#34;\u0026gt; \u0026lt;dhcp\u0026gt; \u0026lt;range start=\u0026#34;192.168.122.1\u0026#34; end=\u0026#34;192.168.122.253\u0026#34;/\u0026gt; \u0026lt;/dhcp\u0026gt; \u0026lt;/ip\u0026gt; \u0026lt;/network\u0026gt; Then run the following commands to activate the network:\nvirsh net-define default.xml virsh net-start default virsh net-autostart default virsh net-list --all We are going with a class C 192.168.122.0/24 subnet because it\u0026rsquo;s already the default, and because it\u0026rsquo;s very distinct from class A, which is what we will be using for the other subnets. Check that the interface virbr0 exists:\nip a | grep virbr0 6: virbr0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 inet 192.168.122.254/24 brd 192.168.122.255 scope global virbr0 You will notice that the host, which will also be the gateway to the Proxmox VMs, is using IP ending in .254. That\u0026rsquo;s a personal preference, as I like when we have vm-pve1, vm-pve2 and vm-pve3 to have their IPs ending in .1, .2 and .3 respectively. Keeps everything easy to remember and tidy.\n2.3.2. Define the Other Subnets We will now configure the bridges which will simulate a virtual switch with separate VLANs. This is how we will configure the subnets:\nHA/Corosync: will be on subnet 10.0.253.0/24 and bridge ha-br. Ceph: will be on subnet 10.0.254.0/24 and bridge ceph-br. Storage: will be on subnet 10.0.255.0/24 and bridge st-br. VMs: These are the VMs which will run inside the virtual Proxmox nodes, they will be on subnet 10.0.X.0/24 (where X is something which doesn\u0026rsquo;t conflict with your home network, maybe 1 or 10) and bridge vm-br. The logic for these addresses is that we would \u0026ldquo;backbone\u0026rdquo; infrastructure to be distinct from where our VMs are. I also choose IPs which will probably not collide with whatever configuration you have at home. I also think that is a good practice for production environments, to choose private IPs which will likely not conflict with home routers. In the case of the bridge vm-br: in a production environment you would probably have multiple VLANs and subnets there rather than just one, potentially servicing thousands of devices.\nNow you can copy the .xml bellow into your ~/Projects/virtual-proxmox-lab/host/configs directory, using vim or whatever you prefer to create each file:\nfile: ha-br.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;ha-br\u0026lt;/name\u0026gt; \u0026lt;bridge name=\u0026#34;ha-br\u0026#34;/\u0026gt; \u0026lt;ip address=\u0026#34;10.0.253.254\u0026#34; netmask=\u0026#34;255.255.255.0\u0026#34;/\u0026gt; \u0026lt;/network\u0026gt; file: ceph-br.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;ceph-br\u0026lt;/name\u0026gt; \u0026lt;bridge name=\u0026#34;ceph-br\u0026#34;/\u0026gt; \u0026lt;ip address=\u0026#34;10.0.254.254\u0026#34; netmask=\u0026#34;255.255.255.0\u0026#34;/\u0026gt; \u0026lt;/network\u0026gt; file: st-br.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;st-br\u0026lt;/name\u0026gt; \u0026lt;bridge name=\u0026#34;st-br\u0026#34;/\u0026gt; \u0026lt;ip address=\u0026#34;10.0.255.254\u0026#34; netmask=\u0026#34;255.255.255.0\u0026#34;/\u0026gt; \u0026lt;/network\u0026gt; file: vm-br.xml\n\u0026lt;network\u0026gt; \u0026lt;name\u0026gt;vm-br\u0026lt;/name\u0026gt; \u0026lt;bridge name=\u0026#34;vm-br\u0026#34;/\u0026gt; \u0026lt;/network\u0026gt; After you\u0026rsquo;re done, time to enable all of them. Let\u0026rsquo;s use a for loop to save us some time:\nfor net in ha-br ceph-br st-br vm-br; do virsh net-define ${net}.xml virsh net-start ${net} virsh net-autostart ${net} done Now when you check with virsh net-list --all this is what you should see:\n❯ virsh net-list --all Name State Autostart Persistent -------------------------------------------- ceph-br active yes yes default active yes yes ha-br active yes yes st-br active yes yes vm-br active yes yes And another quick check on your actual network interfaces:\nfor net in ha-br ceph-br st-br vm-br; do ip a | grep ${net} done Which should give you something like this:\n6: virbr0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 inet 192.168.122.254/24 brd 192.168.122.255 scope global virbr0 7: ha-br: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 inet 10.0.253.254/24 brd 10.0.253.255 scope global ha-br 8: ceph-br: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 inet 10.0.254.254/24 brd 10.0.254.255 scope global ceph-br 9: st-br: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 inet 10.0.255.254/24 brd 10.0.255.255 scope global st-br 10: vm-br: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc htb state DOWN group default qlen 1000 3. Wrapping up Install tree with sudo pacman -S tree. Then run tree -L 2 from the project\u0026rsquo;s folder and you should be looking at something like this:\n$ tree -L 2 . ├── diagrams ├── guests │ ├── pbs │ ├── pve-01 │ ├── pve-02 │ └── pve-03 ├── host │ ├── configs │ │ ├── ceph-br.xml │ │ ├── default.xml │ │ ├── ha-br.xml │ │ ├── st-br.xml │ │ └── vm-br.xml │ └── scripts This is it for network creation. I hate to end it in a cliffhanger, but such is life. Next week we will be configuring storage and creating the Proxmox VMs. Maybe we will will also be installing one instance. If you have some experience you could already go ahead and experiment with what we\u0026rsquo;ve built so far. I hope to see you again soon!\nGod bless you.\n","permalink":"http://localhost:1313/posts/vlab_part-1/","summary":"\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\u003ch3 id=\"11-lab-design\"\u003e1.1. Lab Design\u003c/h3\u003e\n\u003cp\u003eI have some cool experience deploying and configuring a Proxmox cluster with High Availability and fully redundant Ceph in production. I thought it\u0026rsquo;d be cool to share my insights. Since I have been meaning to redeploy my lab on my laptop to test some things, I thought this is a good time. To start, this post will be about setting up the lab environment and install one Proxmox instance. I\u0026rsquo;m going to write a few other tutorials as I go. You are free to use the content here as you see fit. I would appreciate being credited, but that\u0026rsquo;s not a requirement.\u003c/p\u003e","title":"Virtual Proxmox Lab (Part 1) - Setting up your Environment"},{"content":"Introduction Systemd service units are files which define a service, and when/how it\u0026rsquo;s started. They will normally refer to a program or script to be executed. There are 3 fundamental sections on a service file:\n[Unit] is where the description resides, as well as the conditions to initialize the service, such as requirements and dependencies.\n[Service] is where the parameters which determine how to run the service reside.\n[Install] is how to enable the unit. It\u0026rsquo;s optional, but normally necessary.\nThe [Unit] Section Let\u0026rsquo;s go through some fundamental fields and what goes in them:\nDescription= A human-readable description. Example: Description=Apache Web Server.\nAfter= It determines after which units this unit will be started. Example: After=network.target.\nBefore= It determines before which units this unit will be started. Example: Before=httpd.service.\nRequires= It determines a hard dependency, without which this unit fails. Example: Requires=network.target.\nWants= This is a soft dependency. It should be started before this unit, but not required. Example: Wants=network-online.target.\nBindsTo= This is a stronger dependency than Requires=. Stops if this dependency stops. Example: BindsTo=dev-sda1.device.\nTo summarize, there are fields which determine ordering: After= \u0026amp; Before=, and there are fields which determine dependency: Requires=, Wants= \u0026amp; BindsTo=.\nThe [Service] Section These are the most usual fields which determine how the service:\nType= How Systemd tracks the service. Example: Type=simple.\nExecStart= Command used to start the service. Example: ExecStart=/usr/sbin/httpd - DFOREGROUND\nExecStop= Command to stop. This is optional, SIGTERM is default. Example: ExecStop=/usr/sbin/httpd -k stop.\nExecReload= Command to reload configuration. Example: ExecReload=/bin/kill -HUP $MAINPID\nRestart= When to auto-restart. Example: Restart=on-failure\nRestartSec= Delay before attempting to restart. Example: RestartSec=5\nUser= Run this service as this user. Example: User=apache\nGroup= Run this service as this user. Example: Group=apache\nWorkingDirectory= Sets the working directory for the service. Example: WorkingDirectory=/var/www\nEnvironment= Set environment variable. Example: Environment=LANG=en_US.UTF-8\nEnvironmentFile= Load environment variables from file. EnvironmentFile=/etc/sysconfig/httpd\nStandardOutput= Where to send STDOUT. Example: StandardOutput=journal\nStandardError= Where to send STDERR. Example: StandardError=journal\nService Types These are possible values which go in the Type= field:\nsimple This is the default. Process started by ExecStart= is the main process. This is used when the service runs in the foreground, doesn\u0026rsquo;t fork.\nforking Process forks into background. Systemd waits for parent to exit. Tradition daemons that fork. Use PIDFile=.\noneshot Process runs and exits. Service is active while running. This is for scripts that do one task and exit.\nnotify Like simple, but service signals readiness via sd_notify(). This is for services built with Systemd notification.\ndbus Service is ready whenit acquires its D-Bus name. D-Bus activated services.\nidle Like simple, but waits until other jobs finish. This is for low-priority startup tasks.\nRestart Policies These are possible values which go in the Restart= field:\nno Never. This is default.\non-success Clean exit (exit code 0).\non-failure Non-zero exit, signal, timeout, watchdog.\non-abnormal Signal, timeout, watchdog (not exit codes).\non-abort Unclean signal (SIGABRT, SIGSEGV, etc).\non-watchdog Watchdog timeout only.\nalways Always, regardless of exit reason.\nThe [Install] Section The [Install] section tells systemd what happens when you run systemctl enable. It doesn\u0026rsquo;t affect runtime behavior, but only controls where symlinks get created:\nWantedBy= Target that \u0026ldquo;wants\u0026rdquo; this unit (creates symlink). Think of it as \u0026ldquo;start my service when this target activates, but if it fails, the target still succeeds\u0026rdquo;. Example: WantedBy=multi-user.target.\nRequiredBy= Target that \u0026ldquo;requires\u0026rdquo; this unit. Think of it as \u0026ldquo;start my service when this target activates, and if it fails, the target fails too\u0026rdquo;. Example: RequiredBy=graphical.target.\nAlias= Alternative names for the unit. Example: Alias=www.service.\nHow enable works: When you run systemctl enable myservice, Systemd reads the [Install] section and creates a symlink:\n/etc/systemd/system/multi-user.target.wants/myservice.service -\u0026gt; /etc/systemd/system/myservice.service Vendor units live in /usr/lib/systemd/system\nIf there is no [Install] section, the unit can\u0026rsquo;t be enabled/disabled, but runs only as a dependency.\nMinimal Service Template [Unit] Description=My Custom Service After=network.target [Service] Type=simple ExecStart=/usr/local/bin/myscript.sh Restart=on-failure [Install] WantedBy=multi-user.target Follow Along Example Let\u0026rsquo;s set up a simple service to make sure that the concepts stick. First, write a simple shell script which just does something. If you want the easy way, as root, copy and paste the following heredoc:\ncat \u0026gt; /usr/local/bin/myservice.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash while true; do echo \u0026#34;$(date): Running\u0026#34; \u0026gt;\u0026gt; /tmp/myservice.log sleep 10 done EOF chmod +x /usr/local/bin/myservice.sh Test if your script works by running it with /usr/local/bin/myservice.sh. It will make the terminal busy. Press CTRL+C to stop it and check like this:\n[root@server1 ~]# cat /tmp/myservice.log Sun Jan 11 06:42:17 PM GMT 2026: Running If it worked like in this example, you\u0026rsquo;re gold. Next, choose your editor and create a unit file for myservice.sh. In my case I prefer vim (you should too): vim /etc/systemd/system/myservice.service:\n[Unit] Description=A simple service which logs time every 10 sec After=network.target [Service] Type=simple ExecStart=/usr/local/bin/myservice.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Alternatively, if you\u0026rsquo;re feeling lazy, copy and paste the heredoc:\ncat \u0026gt; /etc/systemd/system/myservice.service \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [Unit] Description=A simple service which logs time every 10 sec After=network.target [Service] Type=simple ExecStart=/usr/local/bin/myservice.sh Restart=on-failure RestartSec=3 [Install] WantedBy=multi-user.target EOF To make sure that Systemd recognizes the service, run:\nsystemctl daemon-reload\nsystemctl start myservice.service\nsystemctl status myservice.service to see that it\u0026rsquo;s running.\nIf you want it to enable it to always run when booting: systemctl enable myservice.service.\nYou could check that the service is running in two ways:\ntail -f /tmp/myservice.log and you will see a new line every 10 seconds.\njournalctl -u myservice -f (this is fancier) and you should see logs coming up every 10 seconds, plus the history of the service.\nTo stop it, run systemctl stop myservice.service.\nConclusion In this lesson you have learned what the basic components of a Systemd service unit file are, and how to create a simple service. I hope that this was instructive. More on Systemd will follow. Next week I hope to post about Systemd timers. Thank you for your read!\n","permalink":"http://localhost:1313/posts/systemd-service-units/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSystemd service units are files which define a service, and when/how it\u0026rsquo;s started. They will normally refer to a program or script to be executed. There are 3 fundamental sections on a service file:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e[Unit]\u003c/code\u003e is where the description resides, as well as the conditions to initialize the service, such as requirements and dependencies.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e[Service]\u003c/code\u003e is where the parameters which determine how to run the service reside.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e[Install]\u003c/code\u003e is how to enable the unit. It\u0026rsquo;s optional, but normally necessary.\u003c/p\u003e","title":"Systemd Service Units"},{"content":"I\u0026rsquo;ve spent over fifteen years building and supporting Linux and network infrastructure. I\u0026rsquo;ve built teams from scratch, worn every hat there is, and written more internal documentation than I can count. But I never wrote publicly. Not because I had nothing to say, but because the timing wasn\u0026rsquo;t right.\nIt\u0026rsquo;s right now.\nThis blog exists because I want to put real work and real thinking out into the world. Technical posts about what I\u0026rsquo;m actually building: Linux, Proxmox, automation, networking. And posts about what I\u0026rsquo;m actually thinking: work ethics, privacy, technology, whatever I\u0026rsquo;m working through at the time.\nWe live in a moment where most content is either generated by machines or optimized for engagement. I think that makes honest writing more valuable, not less. I\u0026rsquo;m not interested in chasing algorithms or packaging takes for clicks. I\u0026rsquo;d rather write something worth reading once than something designed to be scrolled past a thousand times.\nI should say the thing that matters most. I\u0026rsquo;m a Christian. My faith came later in life, but it became the foundation for everything else, how I treat people, the standards I hold my work to, and the way I see the world. I believe a carpenter from Galilee two thousand years ago is the answer to the big questions, and I\u0026rsquo;d invite anyone to look into what He said and why He became the most important person in history. I won\u0026rsquo;t be subtle about that here, but I also won\u0026rsquo;t be obnoxious about it. It\u0026rsquo;s simply the truth as I understand it, and truth is worth sharing.\nIf any of that resonates, welcome. If not, stick around anyway. The technical content is solid and the opinions are free.\n","permalink":"http://localhost:1313/posts/why-have-a-blog/","summary":"\u003cp\u003eI\u0026rsquo;ve spent over fifteen years building and supporting Linux and network infrastructure. I\u0026rsquo;ve built teams from scratch, worn every hat there is, and written more internal documentation than I can count. But I never wrote publicly. Not because I had nothing to say, but because the timing wasn\u0026rsquo;t right.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s right now.\u003c/p\u003e\n\u003cp\u003eThis blog exists because I want to put real work and real thinking out into the world. Technical posts about what I\u0026rsquo;m actually building: Linux, Proxmox, automation, networking. And posts about what I\u0026rsquo;m actually thinking: work ethics, privacy, technology, whatever I\u0026rsquo;m working through at the time.\u003c/p\u003e","title":"Why Have a Blog?"},{"content":"I\u0026rsquo;m Thomas. Brazilian by birth, shaped by years in Germany, and currently settled in Ireland with my family.\nMy faith in Christ is central to who I am. It informs my ethics, how I treat people, and the standard I hold my work to. I believe it\u0026rsquo;s true, and I think truth is worth sharing.\nI\u0026rsquo;ve spent over fifteen years in infrastructure engineering, mostly Linux, networking, and virtualization. I\u0026rsquo;ve worked across telecoms, logistics, e-commerce, and healthcare, from night shifts in a network operations center to architecting systems from scratch. The companies and titles changed, but the work stayed the same: make things work, keep them running, and leave them better than you found them.\nI started this blog because I wanted a place to write honestly. The technical posts cover what I\u0026rsquo;m actually working on: Linux administration, Proxmox, Ansible, shell scripting, problems I\u0026rsquo;ve solved and how. The rest is whatever I\u0026rsquo;m thinking through: work ethics, privacy, technology and society, the occasional opinion that might not be popular.\nNo fluff, no engagement bait. I write what I learn and what I think.\nIf you want to get in touch, blog@lutkus.net is the way.\n","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eI\u0026rsquo;m Thomas. Brazilian by birth, shaped by years in Germany, and currently settled in Ireland with my family.\u003c/p\u003e\n\u003cp\u003eMy faith in Christ is central to who I am. It informs my ethics, how I treat people, and the standard I hold my work to. I believe it\u0026rsquo;s true, and I think truth is worth sharing.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve spent over fifteen years in infrastructure engineering, mostly Linux, networking, and virtualization. I\u0026rsquo;ve worked across telecoms, logistics, e-commerce, and healthcare, from night shifts in a network operations center to architecting systems from scratch. The companies and titles changed, but the work stayed the same: make things work, keep them running, and leave them better than you found them.\u003c/p\u003e","title":"About"}]